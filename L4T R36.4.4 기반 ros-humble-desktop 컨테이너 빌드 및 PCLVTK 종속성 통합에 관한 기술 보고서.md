# L4T R36.4.4 기반 ros-humble-desktop 컨테이너 빌드 및 PCL/VTK 종속성 통합에 관한 기술 보고서

## I. 기술 요약 브리핑 (Technical Executive Summary)

본 보고서는 NVIDIA Jetson AGX Orin 호스트(L4T R36.4.4)에서 `dusty-nv/jetson-containers` 프레임워크를 사용하여 `ros-humble-desktop` 컨테이너를 빌드하는 엔지니어링 프로토콜을 제시합니다. 특히, 이 빌드에는 `libvtk9-dev`, `qtbase5-dev`, `qt5-qmake`, `libpcl-dev` 등 3D 인식 및 시각화에 필수적인 시스템 라이브러리를 통합하는 것을 목표로 합니다.

단순한 명령어 실행으로는 이 목표를 달성할 수 없습니다. 분석 결과, 이 작업은 두 가지의 심각하고 상호 독립적인 빌드 실패 지점을 동시에 해결해야 하는 복잡한 과제임이 확인되었습니다.

1. **1차 장애물 (기본 빌드 실패):** `ros:humble-desktop` 패키지의 현재 `jetson-containers` 빌드 스크립트에는 `E: Unable to locate package rti-connext--ros` 1 오류를 유발하는 알려진 문제가 있습니다.
2. **2차 장애물 (종속성 충돌):** 사용자가 요청한 `libpcl-dev` 패키지를 `apt`를 통해 설치하려는 시도는 `jetson-containers`가 ROS 브리지 패키지(예: `pcl_conversions`)를 소스에서 빌드하는 방식과 근본적으로 충돌합니다. 이는 `E: Unmet dependencies` 오류 3로 이어져, 시스템 PCL과 ROS PCL 간의 호환성이 깨지게 됩니다.

본 보고서는 이러한 문제를 해결하기 위해 다음과 같은 2단계 엔지니어링 솔루션을 제공합니다.

- **1단계: `jetson-containers` 패치 및 기본 이미지 빌드:** `rti-connext` 관련 패키지를 `rosdep` 설치에서 명시적으로 제외하도록 `jetson-containers`의 내부 빌드 스크립트(`Dockerfile.ros2`)를 직접 수정(패치)합니다. 그 후, 패치된 스크립트를 사용하여 안정적인 `ros-humble-desktop` 기본 이미지를 빌드합니다.
- **2단계: 다계층 Dockerfile을 통한 종속성 통합:** 새로 빌드된 `ros-humble-desktop` 이미지를 `FROM`으로 사용하는 별도의 커스텀 `Dockerfile`을 작성합니다. 이 파일 내에서 (a) `apt`를 통해 `libpcl-dev`, `libvtk9-dev` 등 *시스템* 라이브러리를 설치한 뒤, (b) `perception_pcl` 스택 4을 소스에서 직접 다운로드하여 방금 설치한 *시스템* 라이브러리를 대상으로 `colcon build`를 실행합니다.

이 접근 방식은 `apt` 패키지 관리와 소스 빌드 간의 충돌을 해결하는 유일하고도 가장 견고한 방법이며, 사용자의 특정 요구사항을 충족하는 완전한 기능의 ROS 2 Humble 환경을 제공합니다.



## 도커 설치



```
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker $USER
```

로그아웃 다시 로그인

```
newgrp docker
```





## II. 1단계: 호스트 시스템(L4T) 버전 식별 및 해석

컨테이너는 호스트 시스템의 커널과 NVIDIA 드라이버를 직접 공유합니다. 따라서 `jetson-containers` 프레임워크가 올바른 기본 이미지를 선택하고 빌드할 수 있도록 호스트의 L4T 버전을 정확하게 식별하는 것이 필수적입니다.  

### A. L4T와 JetPack의 개념적 구분

- **L4T (Linux for Tegra):** Jetson Linux 드라이버 패키지(BSP)로, Linux 커널, 부트로더, NVIDIA 드라이버 및 Ubuntu 기반 루트 파일 시스템을 포함하는 실제 **운영 체제**입니다. `jetson-containers`의 `autotag`  스크립트가 호환성을 결정하는 데 사용하는 핵심 버전입니다 (예: `r36.2.0`).  
- **JetPack:** L4T를 포함하여 CUDA, cuDNN, TensorRT, VPI, OpenCV 등 전체 SDK를 포함하는 상위 수준의 **소프트웨어 번들**입니다. L4T 버전은 특정 JetPack 버전과 1:1로 매핑됩니다.  

### B. 신뢰할 수 있는 버전 확인 방법론 (권장 순)

Jetson AGX Orin과 같은 최신 시스템에서는 `apt` 패키지 관리자를 통해 시스템이 업데이트되므로, 현재 설치된 패키지 버전을 확인하는 것이 가장 정확합니다.

#### 1. `nvidia-l4t-core` 패키지 쿼리 (최우선 권장)

이 방법은 `apt` 패키지 관리자를 통해 설치된 L4T 핵심 구성요소의 버전을 직접 쿼리합니다. 이는 `apt` 기반으로 업데이트되는 현대적인 JetPack 릴리스(JetPack 4 후기 버전 및 모든 5, 6)에서 시스템의 "현재 상태"를 반영하는 가장 정확한 방법입니다.  

**명령어:**

```bash
dpkg-query --show nvidia-l4t-core
```

**출력 예시:** `nvidia-l4t-core 36.4.4-0` (이 경우 L4T 36.4.4를 의미)

#### 2. `nvidia-jetpack` 메타패키지 확인

이는 JetPack SDK 메타패키지의 버전을 보여주며, L4T 버전과 직접적인 상관관계가 있습니다. 또한 설치된 종속성(CUDA, CUDNN 등)을 확인하는 데 유용합니다.  

**명령어:**

```Bash
sudo apt-cache show nvidia-jetpack 
# 또는
sudo apt-cache show nvidia-jetpack | grep "Version:"
```

**출력 예시:** `Version: 6.2.1-b3` (JetPack 6.2.1)

#### 3. `nv_tegra_release` 파일 (레거시/보조)

이 파일은 시스템이 처음 플래시될 때의 L4T 릴리스 정보를 포함합니다. 그러나 에서 지적하듯이, `apt`를 통해 시스템이 업데이트된 경우 이 파일은 더 이상 존재하지 않거나  부정확한(초기 플래시 상태) 정보를 담고 있을 수 있습니다. Orin과 같은 최신 시스템에서는 `dpkg` 방법을 신뢰해야 합니다.  

**명령어:**

```Bash
head -n 1 /etc/nv_tegra_release
```

*참고:* 의 `R32` (JetPack 4) 예시는 이것이 구형 시스템(예: Nano)에서 더 일반적이었음을 시사합니다.  

### C. 보조 정보: CUDA 버전 확인

CUDA 버전은 JetPack 버전과 밀접하게 연관되어 있으며 , `jetson-containers`가 올바른 CUDA 지원 컨테이너를 선택하는 데 사용되므로  L4T 버전과 함께 확인하는 것이 좋습니다.  

**명령어:**

```Bash
nvcc --version
```

### D. 버전 해석 및 다음 단계 결정

사용자는 이 섹션의 명령어를 실행하여 자신의 L4T 버전(예: `36.x.x`)을 정확히 식별해야 합니다. 이 버전 번호는 다음 3단계에서 사용자의 빌드 전략을 결정하는 데 매우 중요합니다.

버전 확인에는 "진실의 계층"이 존재합니다. `/etc/nv_tegra_release` 는 "플래시된" 진실을, `dpkg -l nvidia-l4t-core` 는 "현재" 진실을 나타냅니다. 컨테이너는 호스트의 *현재* 커널 및 드라이버와 상호작용하므로, L4T 35.x 및 36.x와 같은 `apt` 관리 시스템에서는 `dpkg`가 유일한 진실의 원천(source of truth)입니다. `autotag` 와 같은 `jetson-containers` 도구는 이 "현재" L4T 버전에 맞는 컨테이너(예: `r36.2.0`)를 찾으므로, `dpkg`의 결과를 신뢰해야 합니다.  

### B. 핵심 전제 조건: NVIDIA Docker 런타임 설정

`jetson-containers` 빌드 실패의 가장 흔한 원인은 Docker가 호스트의 GPU 및 Tegra 라이브러리에 접근하지 못하는 것입니다.

- **문제점:** `jetson-containers` 빌드 프로세스는 `docker build` 단계에서 호스트의 NVIDIA 라이브러리(예: CUDA, cuDNN, Tegra 링커 라이브러리)에 접근해야 합니다. `jetson-inference` 빌드 실패에 대한 GitHub 이슈 는 L4T R36.4.4에서 Tegra 라이브러리(`libnvrm_gpu.so` 등 )가 빌드 컨테이너에 마운트되지 않아 링커 오류가 발생한다고 구체적으로 명시합니다.  

**솔루션:** `docker run` 시점에 `--runtime nvidia` 를 추가하는 것만으로는 충분하지 않습니다. Docker *데몬* 자체가 `nvidia`를 기본 런타임으로 사용하도록 설정해야 합니다.  

**검증:** `dusty-nv/jetson-containers` 리포지토리의 `install.sh`  스크립트는 이 설정을 시도하지만, 반드시 수동으로 확인해야 합니다.  

**필수 검증 단계:** 빌드를 시도하기 전, 호스트 터미널에서 다음 명령어를 실행하십시오:

```
sudo docker info | grep -i "Default Runtime"
```

**예상되는 정상 출력:** `Default Runtime: nvidia`. 만약 `runc`으로 표시된다면, `jetson-containers/docs/setup.md` 의 "Docker Default Runtime" 섹션 의 지침에 따라 `/etc/docker/daemon.json` 파일을 수정하고 Docker 데몬을 재시작해야 합니다. 이 단계를 생략하면 빌드는 100% 실패합니다.  



## jetson agx orin R36.4.4 에 nvidia container toolkit 설치 및 구성







Jetson AGX Orin의 **L4T R36.4.4 (JetPack 6.x 기반)** 환경에서 **NVIDIA Container Toolkit**을 설치하고 구성하는 과정은 이전 버전(JetPack 5 이하)과는 조금 다르며, 표준 Ubuntu Server와 더 유사해졌습니다. JetPack 6부터는 클라우드 네이티브 환경 지원이 강화되어 `nvidia-smi` 등도 컨테이너 내부에서 더 원활하게 작동합니다.

다음은 단계별 설치 및 구성 가이드입니다.

------



### 1. 사전 준비 및 확인



설치를 시작하기 전에 현재 L4T 버전을 확인하여 환경이 맞는지 점검합니다.

Bash

```
cat /etc/nv_tegra_release
# 출력 결과에 R36 (rev 4.4) 등이 포함되어 있는지 확인
```



### 2. 저장소(Repository) 설정



NVIDIA Container Toolkit 패키지 저장소를 시스템에 추가해야 합니다. JetPack 6(Ubuntu 22.04) 환경에서는 다음 명령어를 사용하여 GPG 키와 저장소 리스트를 설정합니다.

Bash

```
# 1. GPG 키 다운로드 및 저장
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
```



### 3. 패키지 설치



저장소 정보를 업데이트하고 툴킷을 설치합니다.

Bash

```
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
```



### 4. Docker 런타임 구성 (핵심 단계)



설치만으로는 Docker가 GPU를 인식하지 못합니다. `nvidia-ctk` 명령어를 사용하여 Docker 데몬(`daemon.json`)이 NVIDIA 런타임을 사용하도록 구성해야 합니다.

Bash

```
# Docker 런타임 구성 적용
sudo nvidia-ctk runtime configure --runtime=docker

# 변경 사항 적용을 위해 Docker 서비스 재시작
sudo systemctl restart docker
```

> **참고:** 이 명령은 `/etc/docker/daemon.json` 파일을 자동으로 수정하여 `"default-runtime": "nvidia"` 설정을 추가하거나 런타임 경로를 연결해 줍니다.









## dusty-nv/docker-containers/docs/setup.md

Install the latest version of JetPack 4 on Nano/TX1/TX2, JetPack 5 on Xavier, or JetPack 6 on Orin.  The following versions are supported:

* JetPack 4.6.1+ (>= L4T R32.7.1)
* JetPack 5.1+  (>= L4T R35.2.1)
* JetPack 6.0 DP (L4T R36.2.0)
* JetPack 6.2 DP (L4T R36.4.4)

> [!NOTE]  
> <sup>- Building on/for x86 platforms isn't supported at this time (one can typically install/run packages the upstream way there)</sup><br>
> <sup>- The below steps are optional for [pulling/running](/docs/run.md) existing container images from registry, but recommended for building containers locally.</sup>

### Clone the Repo

This will download and install the jetson-containers utilities:

```bash
git clone https://github.com/dusty-nv/jetson-containers
bash jetson-containers/install.sh
```

The installer script will prompt you for your sudo password, and will setup some Python [requirements](/requirements.txt) and add tools like [`autotag`](/docs/run.md#autotag) the `$PATH` by linking them under `/usr/local/bin` (if you move your jetson-containers repo, run this step again)

If you are only running containers and already have enough disk space on your root drive to download them, you may be able to skip the rest of the steps below, but they are recommended best-practices and should be followed when building your own containers.

### Docker Default Runtime

If you're going to be building containers, you need to set Docker's `default-runtime` to `nvidia`, so that the NVCC compiler and GPU are available during `docker build` operations.  Add `"default-runtime": "nvidia"` to your `/etc/docker/daemon.json` configuration file before attempting to build the containers:

``` json
{
    "runtimes": {
        "nvidia": {
            "path": "nvidia-container-runtime",
            "runtimeArgs": []
        }
    },

    "default-runtime": "nvidia"
}
```

Then restart the Docker service, or reboot your system before proceeding:

```bash
$ sudo systemctl restart docker
```

You can then confirm the changes by looking under `docker info`

```bash
$ sudo docker info | grep 'Default Runtime'
Default Runtime: nvidia
```

### Relocating Docker Data Root

Containers can take up a lot of disk space.  If you have external storage available, it's advised to relocate your Docker container cache to the larger drive (NVME is preferred if possible).  If it's not already, get your drive formatted as ext4 and so that it's mounted at boot (i.e. it should be in `/etc/fstab`).  If it's not automatically mounted at boot before the Docker daemon starts, the directory won't exist for Docker to use.

Copy the existing Docker cache from `/var/lib/docker` to a directory on your drive of choice (in this case, `/mnt/docker`):

```bash
$ sudo cp -r /var/lib/docker /mnt/docker
```

Then add your directory as `"data-root"` in `/etc/docker/daemon.json`:

``` json
{
    "runtimes": {
        "nvidia": {
            "path": "nvidia-container-runtime",
            "runtimeArgs": []
        }
    },

    "default-runtime": "nvidia",
    "data-root": "/mnt/docker"
}
```

Then restart the Docker service, or reboot your system before proceeding:

```bash
$ sudo systemctl restart docker
```

You can then confirm the changes by looking under `docker info`

```bash
$ sudo docker info | grep 'Docker Root Dir'
Docker Root Dir: /mnt/docker
...
Default Runtime: nvidia
```

That directory will also now have had it's permissions changed to root-access only by the Docker daemon.

### Mounting Swap

If you're building containers or working with large models, it's advisable to mount SWAP (typically correlated with the amount of memory in the board).  Run these commands to disable ZRAM and create a swap file:

``` bash
sudo systemctl disable nvzramconfig
sudo fallocate -l 16G /mnt/16GB.swap
sudo mkswap /mnt/16GB.swap
sudo swapon /mnt/16GB.swap
```

> If you have NVME storage available, it's preferred to allocate the swap file on NVME.

Then add the following line to the end of `/etc/fstab` to make the change persistent:

``` bash
/mnt/16GB.swap  none  swap  sw 0  0
```

### Disabling the Desktop GUI

If you're running low on memory, you may want to try disabling the Ubuntu desktop GUI.  This will free up extra memory that the window manager and desktop uses (around ~800MB for Unity/GNOME or ~250MB for LXDE)  

You can disable the desktop temporarily, run commands in the console, and then re-start the desktop when desired: 

``` bash
$ sudo init 3     # stop the desktop
# log your user back into the console (Ctrl+Alt+F1, F2, ect)
$ sudo init 5     # restart the desktop
```

If you wish to make this persistent across reboots, you can use the follow commands to change the boot-up behavior:

``` bash
$ sudo systemctl set-default multi-user.target     # disable desktop on boot
$ sudo systemctl set-default graphical.target      # enable desktop on boot
```

### Adding User to Docker Group

Seeing as Ubuntu users aren't by default in the `docker` group, they need to run docker commands with `sudo` (the build tools automatically do this when needed).  Hence you could be periodically asked for your sudo password during builds.  

Instead, you can add your user to the docker group like below:

```bash
sudo usermod -aG docker $USER
```

Then close/restart your terminal (or logout) and you should be able to run docker commands (like `docker info`) without needing sudo.

### Setting the Power Mode

Depending on the power supply source you have available for your Jetson (i.e. wall power or battery), you may wish to put your Jetson in maximum power mode (MAX-N) to attain the highest performance available from your Jetson device.  You can do this with the [`nvpmodel`](https://docs.nvidia.com/jetson/archives/r36.2/DeveloperGuide/SD/PlatformPowerAndPerformance/JetsonOrinNanoSeriesJetsonOrinNxSeriesAndJetsonAgxOrinSeries.html#power-mode-controls) command-line tool, or from the Ubuntu desktop via the [nvpmodel GUI widget](https://docs.nvidia.com/jetson/archives/r36.2/DeveloperGuide/SD/PlatformPowerAndPerformance/JetsonOrinNanoSeriesJetsonOrinNxSeriesAndJetsonAgxOrinSeries.html#nvpmodel-gui) (or by using [`jtop`](https://github.com/rbonghi/jetson_stats) from jetson-stats)

```bash
# check the current power mode
$ sudo nvpmodel -q
NV Power Mode: MODE_30W
2

# set it to mode 0 (typically the highest)
$ sudo nvpmodel -m 0

# reboot if necessary, and confirm the changes
$ sudo nvpmodel -q
NV Power Mode: MAXN
0
```

See [here](https://docs.nvidia.com/jetson/archives/r36.2/DeveloperGuide/SD/PlatformPowerAndPerformance/JetsonOrinNanoSeriesJetsonOrinNxSeriesAndJetsonAgxOrinSeries.html#supported-modes-and-power-efficiency) for a table of the power modes available for the different Jetson devices, and for documentation on the [`nvpmodel`](https://docs.nvidia.com/jetson/archives/r36.2/DeveloperGuide/SD/PlatformPowerAndPerformance/JetsonOrinNanoSeriesJetsonOrinNxSeriesAndJetsonAgxOrinSeries.html#power-mode-controls) tool.









## 클론 후

jetson-containers에서

./install.sh

pip3 install -r requirements.txt



한다.



# ~/.bashrc 

```Bash
# jetson-containers 설정
export JETSON_CONTAINERS_HOME=/your/jetson-containers
export PATH=$PATH:$JETSON_CONTAINERS_HOME
```

source ~/.bashrc





## II. 빌드 환경 분석 및 전제 조건 검증



성공적인 빌드를 위해서는 호스트 환경에 대한 정확한 이해와 구성이 선행되어야 합니다. 사용자의 L4T R36.4.4 환경은 `jetson-containers` 생태계에서 특정한 의미를 가지며, 사소한 설정 오류가 치명적인 빌드 실패로 이어질 수 있습니다.



### A. 시스템 아키텍처: L4T R36.4.4 (JetPack 6.x)의 의미



사용자가 명시한 `L4T R36.4.4` 1는 단순한 버전 번호가 아니라, 빌드 전략 전체를 결정하는 핵심 아키텍처 사양입니다.

- **L4T-JetPack 매핑:** L4T R36.x 시리즈는 JetPack 6.x 릴리스에 해당합니다.7 L4T R36.4.4는 JetPack 6.2.1에 해당할 가능성이 높습니다.1
- **운영체제:** JetPack 6.x는 Jetson 플랫폼의 OS를 Ubuntu 20.04 (Focal)에서 **Ubuntu 22.04 (Jammy)** 로 업그레이드한 중대한 릴리스입니다.7
- **ROS 호환성:** ROS 2 Humble은 Ubuntu 22.04를 공식 Tier 1 대상으로 설계되었습니다.10

이러한 환경 구성은 기회와 함정을 동시에 제공합니다.

- **기회:** 호스트 OS(Ubuntu 22.04)와 타겟 ROS(Humble)가 기본적으로 호환되므로, JetPack 5(Ubuntu 20.04) 환경에서처럼 3 ROS 전체를 소스에서 빌드할 필요 없이 `apt` 패키지를 활용할 잠재력이 있습니다.
- **함정 (아키텍처 충돌):** 하지만 `jetson-containers`는 `apt` 패키지에 의존하지 않습니다. `dusty-nv` 리포지토리는 JetPack 4, 5, 6 전반에 걸쳐 일관된 경험을 제공하기 위해, 그리고 NVIDIA의 CUDA/TensorRT 가속 라이브러리와의 연동을 보장하기 위해 ROS의 많은 부분을 *의도적으로* 소스에서 빌드합니다.12 이로 인해, 사용자의 시스템(Ubuntu 22.04)이 `ros-humble-*` `apt` 패키지를 지원함에도 불구하고, 컨테이너 내부는 `apt`와 소스 빌드가 혼재된 복잡한 환경이 됩니다. 이것이 바로 IV섹션에서 다룰 `libpcl-dev` 충돌의 근본 원인입니다.



### B. 핵심 전제 조건: NVIDIA Docker 런타임 설정



`jetson-containers` 빌드 실패의 가장 흔한 원인은 Docker가 호스트의 GPU 및 Tegra 라이브러리에 접근하지 못하는 것입니다.

- **문제점:** `jetson-containers` 빌드 프로세스는 `docker build` 단계에서 호스트의 NVIDIA 라이브러리(예: CUDA, cuDNN, Tegra 링커 라이브러리)에 접근해야 합니다.14 `jetson-inference` 빌드 실패에 대한 GitHub 이슈 16는 L4T R36.4.4에서 Tegra 라이브러리(`libnvrm_gpu.so` 등 17)가 빌드 컨테이너에 마운트되지 않아 링커 오류가 발생한다고 구체적으로 명시합니다.16
- **솔루션:** `docker run` 시점에 `--runtime nvidia` 19를 추가하는 것만으로는 충분하지 않습니다. Docker *데몬* 자체가 `nvidia`를 기본 런타임으로 사용하도록 설정해야 합니다.15
- **검증:** `dusty-nv/jetson-containers` 리포지토리의 `install.sh` 19 스크립트는 이 설정을 시도하지만, 반드시 수동으로 확인해야 합니다.

필수 검증 단계:

빌드를 시도하기 전, 호스트 터미널에서 다음 명령어를 실행하십시오:

Bash

```
sudo docker info | grep -i "Default Runtime"
```

예상되는 정상 출력: Default Runtime: nvidia.20

만약 runc으로 표시된다면, jetson-containers/docs/setup.md 21의 "Docker Default Runtime" 섹션 20의 지침에 따라 /etc/docker/daemon.json 파일을 수정하고 Docker 데몬을 재시작해야 합니다. 이 단계를 생략하면 빌드는 100% 실패합니다.



### C. 리포지토리 상태 및 유지보수

- **데이터:** `dusty-nv/jetson-containers`는 NVIDIA 직원이 관리하는 "준공식(semi-official)" 리포지토리입니다.22 관리자(dusty-nv)가 2024년 초에 잠시 물러났으며 23, 이로 인해 JetPack 6 지원 및 버그 수정에 공백이 생겼습니다.24
- **전략적 함의:** 이 리포지토리는 공식 NVIDIA 제품(예: `nvcr.io/nvidia/l4t-base` 25)만큼 안정적이지 않으며, 커뮤니티의 버그 리포트 1에 대한 반응이 늦을 수 있습니다. 이는 사용자가 직면한 `rti-connext` 빌드 실패 2가 리포지토리의 최신 변경 사항을 따라잡지 못한 *유지보수 지연*의 직접적인 결과일 가능성이 높다는 것을 시사합니다. 따라서, 우리는 이 리포지토리를 "완벽한" 솔루션이 아닌, 우리가 *수정해야 할* 코드로 접근해야 합니다.

**표 1: 시스템 구성 및 아키텍처 요약**

| **매개변수**        | **값**                         | **기술적 중요성 및 관련 데이터**                             |
| ------------------- | ------------------------------ | ------------------------------------------------------------ |
| **호스트 디바이스** | Jetson AGX Orin                | 고성능 ARM aarch64 아키텍처 27                               |
| **L4T 버전**        | R36.4.4                        | JetPack 6.x 시리즈(6.1 또는 6.2) 1                           |
| **호스트 OS**       | Ubuntu 22.04 (Jammy)           | JetPack 6.x의 기본 OS.7 ROS Humble의 네이티브 대상 플랫폼.   |
| **타겟 리포지토리** | `dusty-nv/jetson-containers`   | 모듈식, 준공식, 커뮤니티 지원 리포지토리.19                  |
| **타겟 빌드**       | `ros:humble-desktop`           | ROS 2 Humble 데스크톱 버전.19                                |
| **핵심 종속성**     | `libpcl-dev`, `libvtk9-dev` 등 | 3D 처리를 위한 시스템 라이브러리. `apt`를 통한 설치 필요.    |
| **빌드 장애물 1**   | `rti-connext--ros`             | `ros:humble-desktop`의 기본 빌드를 막는 패키지 누락 오류.1   |
| **빌드 장애물 2**   | `Unmet dependencies`           | `libpcl-dev` 설치 시 `ros-humble-pcl-conversions`와의 충돌.3 |

## III. 1단계: `ros:humble-desktop` 기본 빌드 패치

첫 번째 목표는 사용자의 특정 종속성을 추가하기 *전*에, `ros:humble-desktop`의 표준 빌드를 성공시키는 것입니다. 현재 이 빌드는 `rti-connext` 패키지 문제로 인해 실패합니다.

### A. 문제 진단: `E: Unable to locate package rti-connext--ros`

- **오류 시그니처:** `jetson-containers build ros:humble-desktop` 명령을 실행하면 2, 빌드 프로세스 중 `apt-get install` 또는 `rosdep install` 단계에서 다음과 유사한 오류가 발생하며 중단됩니다:

  ```
  E: Unable to locate package rti-connext--ros
  E: Couldn't find any package by glob 'rti-connext--ros'
  E: Couldn't find any package by regex 'rti-connext-*-ros'
  ...
  ROS humble build failed!
  subprocess.CalledProcessError: Command '... docker build...' returned non-zero
  ```

1

- **근본 원인 분석:**
  1. RTI Connext는 ROS 2를 위한 여러 DDS(Data Distribution Service) 구현 중 하나입니다.28
  2. `ros:humble-desktop`은 이 패키지를 `rosdep` 종속성으로 포함하고 있습니다.
  3. 이 특정 패키지는 설치 시 대화형 라이선스 동의가 필요하거나 30, `arm64` 아키텍처의 Ubuntu 22.04 `apt` 저장소에서 이름이 변경되었거나 제거되었을 수 있습니다.
  4. 이러한 대화형 프롬프트 또는 누락된 패키지는 비대화형 `docker build` 환경을 중단시킵니다.
  5. 다행히도 Connext DDS는 ROS 2 Humble의 *필수* 종속성이 아닙니다. `jetson-containers`의 최신 빌드는 기본 RMW로 `rmw_fastrtps_cpp` 또는 `rmw_cyclonedds_cpp`를 사용하도록 설정되어 있습니다.26
  6. 따라서, 가장 효과적인 해결책은 Connext 관련 패키지 설치를 *건너뛰도록* 빌드 스크립트를 수정하는 것입니다.
  7. `jetson-containers`의 ROS 빌드 스크립트는 이미 `rosdep install` 명령어에 `--skip-keys` 플래그를 사용하여 OpenCV와 같은 다른 충돌 가능성이 있는 패키지를 제외하고 있습니다.31 이 목록에 `rti-connext` 관련 키를 추가하기만 하면 됩니다.



### B. 해결 프로토콜: `Dockerfile.ros2` 패치



다음은 `ros:humble-desktop`의 기본 빌드 실패를 해결하기 위한 단계별 지침입니다.

1. **리포지토리 복제 및 디렉토리 이동:**

   Bash

   ```
   git clone https://github.com/dusty-nv/jetson-containers
   cd jetson-containers
   ```

2. **빌드 파일 찾기:** `ros:humble-desktop` 빌드는 `packages/robots/ros/Dockerfile.ros2` 파일에 의해 정의됩니다..2

3. Dockerfile.ros2 수정:

   텍스트 편집기로 packages/robots/ros/Dockerfile.ros2 파일을 엽니다.

   RUN... rosdep install...로 시작하는 긴 RUN 명령어를 찾습니다. 이 명령어는 ROS_DISTRO와 ROS_PKG 인수를 기반으로 ROS를 소스에서 빌드합니다.

   해당 명령어 내에서 rosdep install 부분과 --skip-keys 플래그를 찾습니다. 31

   *기존 (예시):*

   ```
   ...
   
   && rosdep install -y --ignore-src --from-paths src --rosdistro ${ROS_DISTRO} 
   
   --skip-keys "libopencv-dev libopencv-contrib-dev libopencv-imgproc-dev python-opencv python3-opencv" 
   
   && rm -rf /var/lib/apt/lists/* && apt-get clean && 
   
   ...
   ```

   *수정:* `--skip-keys` 목록에 `rti_connext_dds_cmake_module` 및 `rti-connext-dds-6.0.1` 을 추가합니다.

   *수정된 (예시):*

   ```dockerfile
   ...
   
   && rosdep install -y --ignore-src --from-paths src --rosdistro ${ROS_DISTRO} 
   
   --skip-keys "libopencv-dev libopencv-contrib-dev libopencv-imgproc-dev python-opencv python3-opencv rti_connext_dds_cmake_module rti-connext-dds-6.0.1" 
   
   && rm -rf /var/lib/apt/lists/* && apt-get clean && 
   
   ...
   ```

참고: skip-keys에 추가할 정확한 이름은 rosdep 버전에 따라 다를 수 있지만, rti_connext_dds_cmake_module이 가장 일반적인 키입니다.

### C. 패치된 기본 이미지 빌드

1. `jetson-containers` 리포지토리의 루트 디렉토리로 돌아옵니다.

2. `install.sh`를 실행하여 빌드 도구를 설정합니다 (아직 실행하지 않은 경우):

   ```Bash
   bash jetson-containers/install.sh
   ```

3. 패치된 `ros:humble-desktop` 빌드를 실행합니다. 빌드 시간을 줄이기 위해 다른 패키지(예: `pytorch`)는 제외하고, 명시적인 태그 이름을 지정합니다.

   ```Bash
   jetson-containers build --name=ros-humble-base-patched ros:humble-desktop
   ```

   19

이 빌드 프로세스는 호스트 시스템(AGX Orin)의 성능에 따라 30분에서 수 시간까지 소요될 수 있습니다.1 빌드가 성공적으로 완료되면 `ros-humble-base-patched:r36.4.4`와 같은 태그가 지정된 로컬 Docker 이미지가 생성됩니다. (태그는 `autotag` 기능 19에 의해 자동으로 생성됨).

## IV. 2단계: PCL/VTK/Qt 종속성 통합

이제 `rti-connext` 문제가 해결된 깨끗한 `ros-humble-base-patched` 이미지를 확보했습니다. 다음 단계는 사용자의 핵심 요구사항인 4개의 `apt` 패키지를 이 이미지에 추가하는 것입니다.

### A. 문제 진단: `apt`와 소스 빌드 간의 종속성 충돌

- **가장 쉬운 실패 시나리오:** 만약 우리가 1단계에서 빌드한 컨테이너를 실행하고(`docker run...`) 내부에서 `sudo apt install libpcl-dev`를 시도하면, ROS `apt` 저장소가 활성화되어 있지 않기 때문에 3 `ros-humble-pcl-conversions`와 같은 ROS 관련 PCL 패키지를 찾지 못할 수 있습니다.

- **더 복잡한 실패 시나리오:** 만약 `apt` 저장소를 추가하고 `apt install ros-humble-perception-pcl` (또는 `libpcl-dev`에 의존하는 관련 패키지)를 시도하면, GitHub 이슈 3에서 보고된 것과 정확히 동일한 오류가 발생합니다:

  ```
  The following packages have unmet dependencies:
   libpcl-dev : Depends: ros-humble-pcl-conversions but it is not going to be installed 
  E: Unmet dependencies. Try 'apt --fix-broken install'
  ```

- **근본 원인 분석:**

  1. 우리의 `ros-humble-base-patched` 이미지는 ROS를 `apt`를 통해 설치하지 않았습니다. ROS는 `/opt/ros/humble`에 *소스에서 컴파일되어* 설치되었습니다.3
  2. `pcl_conversions` (ROS 메시지 <-> PCL 데이터 변환 라이브러리) 4 역시 소스에서 컴파일되었습니다.
  3. `apt install libpcl-dev`를 실행하면 시스템(Ubuntu)의 PCL 라이브러리가 `/usr/lib/`에 설치됩니다.
  4. `apt` 시스템은 `ros-humble-pcl-conversions`라는 *Debian* 패키지가 이 *시스템* `libpcl-dev`에 의존한다고 알고 있습니다.
  5. 하지만 우리의 컨테이너에는 `ros-humble-pcl-conversions`라는 Debian 패키지가 없습니다. 대신 `/opt/ros/humble/install/lib/`에 *소스에서 컴파일된* `libpcl_conversions.so` 파일이 있습니다.
  6. `apt`는 이 상황을 해결할 수 없습니다. 시스템은 "소스에서 컴파일된 ROS"와 "apt로 설치된 라이브러리" 간의 연결 고리를 알지 못합니다.
  7. **솔루션:** 이 연결고리를 수동으로 만들어야 합니다. `apt`를 통해 *시스템* `libpcl-dev`를 설치한 다음, 이 라이브러리(헤더는 `/usr/include/pcl-1.10/`, 라이브러리는 `/usr/lib/aarch64-linux-gnu/`)를 *직접* 사용하도록 `pcl_conversions`와 `pcl_ros`를 소스에서 *다시 컴파일*해야 합니다.34

### B. 해결 프로토콜: 다계층 통합 Dockerfile

이 솔루션은 `jetson-containers` 디렉토리 *외부*의 새 작업 디렉토리에서 수행됩니다.

1. 새 디렉토리를 만들고 `Dockerfile.custom_ros`라는 이름의 새 파일을 생성합니다.

   ```Bash
   mkdir ~/ros_pcl_build
   cd ~/ros_pcl_build
   nano Dockerfile.custom_ros
   ```

2. **`Dockerfile.custom_ros` 파일 내용:**

   *참고: `ARG BASE_IMAGE`의 태그를 1단계에서 빌드한 이미지의 실제 태그로 교체하십시오. (예: `ros-humble-base-patched:r36.4.4`)*

   ```dockerfile
   # 1단계: 1단계에서 성공적으로 빌드한 패치된 기본 이미지를 사용합니다.
   # 로컬에 빌드된 이미지 태그를 정확히 명시해야 합니다. (docker images | grep ros-humble-base-patched)
   ARG BASE_IMAGE=ros-humble-base-patched:r36.4.4
   FROM ${BASE_IMAGE}
   
   # Dockerfile 내에서 apt-get을 비대화형(non-interactive)으로 실행하도록 설정
   ENV DEBIAN_FRONTEND=noninteractive
   
   # 2단계: 사용자가 요청한 시스템 종속성 설치
   # 이는 ROS와 무관한 '시스템' 라이브러리를 설치합니다.
   # 이것이 PCL, VTK, Qt의 기본이 됩니다.
   RUN sudo apt-get update && \
       sudo apt-get install -y --no-install-recommends \
           libvtk9-dev \
           qtbase5-dev \
           qt5-qmake \
           libpcl-dev \
       && sudo rm -rf /var/lib/apt/lists/*
   
   # 3단계: PCL/ROS 브리지 패키지(perception_pcl)를 소스에서 빌드
   # 이 단계가 'apt'와 '소스 빌드' 간의 충돌을 해결하는 핵심입니다.
   # ROS 환경을 먼저 활성화합니다.
   RUN source /opt/ros/humble/setup.bash && \
       # 새 colcon 작업 공간 생성
       mkdir -p /ros2_ws/src && \
       cd /ros2_ws/src && \
       # PCL 브리지(pcl_conversions, pcl_ros)가 포함된 perception_pcl 리포지토리를 복제합니다.
       # 반드시 'humble' 브랜치를 사용해야 합니다. 
       git clone -b humble https://github.com/ros-perception/perception_pcl.git && \
       # /ros2_ws 디렉토리로 이동
       cd /ros2_ws && \
       # rosdep을 실행하여 perception_pcl의 '시스템' 종속성을 설치합니다.
       # (2단계에서 이미 설치한 libpcl-dev 등은 건너뜁니다)
       # rosdep init은 이미 기본 이미지에 있을 수 있으므로 '|| true'로 오류를 무시합니다.
       sudo rosdep init || true && 
       rosdep update && 
   	rosdep install -y --ignore-src -r --from-paths src && 
   	# 4단계: 새로 설치된 '시스템 libpcl-dev'를 대상으로 PCL 브리지를 컴파일합니다.
   	# 34
   	# colcon build는 /usr/include/pcl-1.10/ 헤더를 자동으로 찾아 연결합니다.
   	colcon build --symlink-install --packages-select pcl_conversions pcl_ros
   	
   # 5단계: 새 작업 공간을 컨테이너의.bashrc에 추가
   # 이렇게 하면 컨테이너가 시작될 때마다 새로 컴파일된 PCL 브리지가 활성화됩니다.
   RUN echo "source /ros2_ws/install/setup.bash" >> ~/.bashrc
   
   # 기본 진입점(Entrypoint)은 기본 이미지에서 상속됩니다.
   CMD ["/bin/bash"]
   # 5단계: 새 작업 공간을 컨테이너의.bashrc에 추가
   # 이렇게 하면 컨테이너가 시작될 때마다 새로 컴파일된 PCL 브리지가 활성화됩니다.
   RUN echo "source /ros2_ws/install/setup.bash" >> ~/.bashrc
   
   # 기본 진입점(Entrypoint)은 기본 이미지에서 상속됩니다.
   CMD ["/bin/bash"]
   
   ```

1. 최종 이미지 빌드:

   Dockerfile.custom_ros가 있는 디렉토리(~/ros_pcl_build)에서 다음 명령어를 실행하여 최종 이미지를 빌드합니다.

   ```Bash
   docker build -t my-jetson-ros-pcl:r36.4.4 -f Dockerfile.custom_ros.
   ```

## V. 최종 실행 및 검증

이제 `my-jetson-ros-pcl:r36.4.4`라는 이름의 최종 이미지가 생성되었습니다. 이 이미지는 L4T R36.4.4, `ros-humble-desktop`, 그리고 `libpcl-dev` 및 `libvtk9-dev`와 완벽하게 호환되는 PCL ROS 브리지 패키지를 포함합니다.

### A. 컨테이너 실행

다음 명령어를 사용하여 GPU 가속(`--runtime nvidia`) 및 호스트 네트워킹(`--net=host`)이 활성화된 컨테이너를 실행합니다.19

```Bash
sudo docker run -it --rm \
    --net=host \
    --runtime=nvidia \
    --ipc=host \
    -v /tmp/.X11-unix:/tmp/.X11-unix \
    -e DISPLAY=$DISPLAY \
    my-jetson-ros-pcl:r36.4.4
```

*(참고: GUI 애플리케이션(예: `rviz2`)을 실행하려면 `-v /tmp/.X11-unix...` 및 `-e DISPLAY...`가 필요합니다.)*

### B. 검증 단계

컨테이너 내부에서 다음을 확인하여 빌드가 성공했는지 검증할 수 있습니다.

1. **ROS 환경 확인:**

   ```Bash
   root@jetson:/# ros2 pkg list | grep pcl
   ```

   *예상 출력:* `pcl_conversions`, `pcl_ros` 5

2. **시스템 PCL 라이브러리 설치 확인:**

   ```Bash
   root@jetson:/# apt list --installed | grep libpcl-dev
   ```

   *예상 출력:* `libpcl-dev/jammy,now... [installed]`

3. **시스템 VTK/Qt 라이브러리 설치 확인:**

   ```Bash
   root@jetson:/# apt list --installed | grep libvtk9-dev
   ```

   *예상 출력:* `libvtk9-dev/jammy,now... [installed]`

**표 2: 빌드 실패 분석 및 해결 프로토콜 요약**

| **단계**          | **문제 식별**                                               | **근본 원인 (Root Cause)**                                   | **해결 프로토콜**                                            |
| ----------------- | ----------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **1 (기본 빌드)** | `E: Unable to locate package rti-connext--ros` 1            | `jetson-containers`의 `Dockerfile.ros2` 스크립트가 누락되었거나 라이선스 문제가 있는 `rosdep` 종속성을 호출함. 31 | `packages/robots/ros/Dockerfile.ros2` 파일을 직접 수정. `rosdep install` 명령어의 `--skip-keys` 목록에 `rti_connext_dds_cmake_module`을 추가. 31 |
| **2 (통합)**      | `E: Unmet dependencies:... ros-humble-pcl-conversions...` 3 | 기본 이미지의 소스 빌드 ROS 패키지(`pcl_conversions`)와 `apt`로 설치하려는 시스템 라이브러리(`libpcl-dev`) 간의 충돌. 34 | 별도의 `Dockerfile` 사용: (1) `apt install libpcl-dev`로 시스템 라이브러리 설치. (2) `perception_pcl`을 소스에서 `colcon build`하여 시스템 라이브러리에 강제 연결. |

## VI. 결론 및 최종 권고 사항

사용자의 요청은 `dusty-nv/jetson-containers` 리포지토리의 두 가지 심각한 버그 및 아키텍처적 충돌을 정확히 드러내는 매우 가치 있는 엔지니어링 과제였습니다.

제시된 2단계 솔루션(1단계: 패치, 2단계: 계층화)은 사용자의 모든 요구사항(`ros-humble-desktop`, L4T R36.4.4, PCL/VTK/Qt)을 충족하는 유일하게 견고한 접근 방식입니다.

`dusty-nv/jetson-containers`와 같은 복잡한 빌드 프레임워크를 사용할 때, 프레임워크 자체를 *직접 수정*(`jetson-containers build...`)하려는 시도보다, 프레임워크를 사용하여 *안정적인 기본 이미지*를 생성한 다음, 표준 `docker build` 38를 사용하여 해당 이미지를 기반으로 *계층화(layering)*하는 것이 훨씬 더 안정적이고 유지보수가 용이합니다. 이 보고서에서 제안한 `Dockerfile.custom_ros` 파일은 향후 `jetson-containers` 리포지토리가 업데이트되더라도 사용자의 PCL/VTK 환경을 안전하게 보호할 것입니다.

#### **Works cited**

1. Issues building ROS2 Humble + PyTorch + Open3D container (JetPack 6.2.1 / L4T 36.4.4) · Issue #1431 · dusty-nv/jetson-containers - GitHub, accessed November 17, 2025, https://github.com/dusty-nv/jetson-containers/issues/1431
2. E: Unable to locate package rti-connext--ros when build ros:humble-desktop on jetson nano orin super · Issue #1316 · dusty-nv/jetson-containers - GitHub, accessed November 17, 2025, https://github.com/dusty-nv/jetson-containers/issues/1316
3. how to use "apt install ros-humble-*" on ros humble docker · Issue #284 · dusty-nv/jetson-containers - GitHub, accessed November 17, 2025, https://github.com/dusty-nv/jetson-containers/issues/284
4. pcl_conversions - ROS Package Overview - ROS Index, accessed November 17, 2025, https://index.ros.org/p/pcl_conversions/
5. perception_pcl - ROS Package Overview - ROS Index, accessed November 17, 2025, https://index.ros.org/p/perception_pcl/
6. Jetson-containers does not build correctly - NVIDIA Developer Forums, accessed November 17, 2025, https://forums.developer.nvidia.com/t/jetson-containers-does-not-build-correctly/319279
7. JetPack SDK 6.0 - NVIDIA Developer, accessed November 17, 2025, https://developer.nvidia.com/embedded/jetpack-sdk-60
8. Jetpack 6.1/6.2 Support · Issue #825 · dusty-nv/jetson-containers - GitHub, accessed November 17, 2025, https://github.com/dusty-nv/jetson-containers/issues/825
9. JetPack SDK 6.0 DP - NVIDIA Developer, accessed November 17, 2025, https://developer.nvidia.com/embedded/jetpack-sdk-60dp
10. Install ros 2 humble on jetson orin : r/ROS - Reddit, accessed November 17, 2025, https://www.reddit.com/r/ROS/comments/14ipjm3/install_ros_2_humble_on_jetson_orin/
11. Proper dev toolchain for ROS2 Humble in dusty's pre-built docker image on a remote Xavier NX - NVIDIA Developer Forums, accessed November 17, 2025, https://forums.developer.nvidia.com/t/proper-dev-toolchain-for-ros2-humble-in-dustys-pre-built-docker-image-on-a-remote-xavier-nx/229400
12. Using ROS2-Humble on a Jetson with Hardware acceleration - NVIDIA Developer Forums, accessed November 17, 2025, https://forums.developer.nvidia.com/t/using-ros2-humble-on-a-jetson-with-hardware-acceleration/334990
13. Ubuntu & ROS2 Version for Jetson Nano 4GB Dev Kit - Rev B01, accessed November 17, 2025, https://forums.developer.nvidia.com/t/ubuntu-ros2-version-for-jetson-nano-4gb-dev-kit-rev-b01/304474
14. Tegra libreries are missing in docker build - Jetson Xavier NX - NVIDIA Developer Forums, accessed November 17, 2025, https://forums.developer.nvidia.com/t/tegra-libreries-are-missing-in-docker-build/288841
15. How can I create my custom containers for Jetson Nano - NVIDIA Developer Forums, accessed November 17, 2025, https://forums.developer.nvidia.com/t/how-can-i-create-my-custom-containers-for-jetson-nano/174053
16. Build fails for jetson-inference on L4T R36.4.4: Tegra libs not mounted in build container · Issue #1185 - GitHub, accessed November 17, 2025, https://github.com/dusty-nv/jetson-containers/issues/1185
17. Building llama_cpp container does not produce the compiled C++ version of llama.cpp executables · Issue #509 · dusty-nv/jetson-containers - GitHub, accessed November 17, 2025, https://github.com/dusty-nv/jetson-containers/issues/509
18. Tegra libraries are missing on Nvidia L4Tbase docker Image - Jetson Nano, accessed November 17, 2025, https://forums.developer.nvidia.com/t/tegra-libraries-are-missing-on-nvidia-l4tbase-docker-image/221218
19. dusty-nv/jetson-containers: Machine Learning Containers for NVIDIA Jetson and JetPack-L4T - GitHub, accessed November 17, 2025, https://github.com/dusty-nv/jetson-containers
20. Cannot build image due to `ImportError: libcublas.so.10: cannot open shared object file: No such file or directory` · Issue #222 · dusty-nv/jetson-containers - GitHub, accessed November 17, 2025, https://github.com/dusty-nv/jetson-containers/issues/222
21. The model you are attempting to pull requires a newer version of Ollama.Hi · Issue #585 · dusty-nv/jetson-containers - GitHub, accessed November 17, 2025, https://github.com/dusty-nv/jetson-containers/issues/585
22. Are L4T containers discontinued? : r/JetsonNano - Reddit, accessed November 17, 2025, https://www.reddit.com/r/JetsonNano/comments/1imtfxm/are_l4t_containers_discontinued/
23. Jetson AI Lab Dead? - NVIDIA Developer Forums, accessed November 17, 2025, https://forums.developer.nvidia.com/t/jetson-ai-lab-dead/341702
24. Repo no longer maintained. · Issue #1270 · dusty-nv/jetson-containers - GitHub, accessed November 17, 2025, https://github.com/dusty-nv/jetson-containers/issues/1270
25. ROS 2 Humble: Docker exec Terminals Can't Communicate on Jetson Orin (JetPack 6), accessed November 17, 2025, https://forums.developer.nvidia.com/t/ros-2-humble-docker-exec-terminals-cant-communicate-on-jetson-orin-jetpack-6/344404
26. Jetson Orin Nano Developer Kit with NVIDIA Docker and ROS2, accessed November 17, 2025, https://forums.developer.nvidia.com/t/jetson-orin-nano-developer-kit-with-nvidia-docker-and-ros2/301103
27. Dusty-nv/jetson-containers - NVIDIA Developer Forums, accessed November 17, 2025, https://forums.developer.nvidia.com/t/dusty-nv-jetson-containers/350386
28. RTI Connext DDS — ROS 2 Documentation: Humble documentation, accessed November 17, 2025, https://docs.ros.org/en/humble/Installation/RMW-Implementations/DDS-Implementations/Working-with-RTI-Connext-DDS.html
29. rti_connext_dds gentoo | Data Distribution Service (DDS) Community RTI Connext Users, accessed November 17, 2025, https://community.rti.com/forum-topic/rticonnextdds-gentoo
30. rti-connext-dds package prevents building ros2 from source in docker image - ROS Answers, accessed November 17, 2025, https://answers.ros.org/question/386509/
31. ROS Humble build fail · Issue #205 · dusty-nv/jetson-containers - GitHub, accessed November 17, 2025, https://github.com/dusty-nv/jetson-containers/issues/205
32. Error building ros2 containers · Issue #160 · dusty-nv/jetson-containers - GitHub, accessed November 17, 2025, https://github.com/dusty-nv/jetson-containers/issues/160
33. Combining Jetson Containers - NVIDIA Developer Forums, accessed November 17, 2025, https://forums.developer.nvidia.com/t/combining-jetson-containers/292883
34. install pcl-conversions when pcl is already installed - Robotics Stack Exchange, accessed November 17, 2025, https://robotics.stackexchange.com/questions/73508/install-pcl-conversions-when-pcl-is-already-installed
35. ubuntu_22 : ROS2_humble :/usr/include/pcl_conversions/pcl_conversions.h:fatal error: pcl/conversions.h: No such file or directory, accessed November 17, 2025, https://robotics.stackexchange.com/questions/102848/ubuntu-22-ros2-humble-usr-include-pcl-conversions-pcl-conversions-hfatal-er
36. Dockerfile: ros1_bridge ROS1/noetic ROS2/humble on l4t-jetpack ..., accessed November 17, 2025, https://forums.developer.nvidia.com/t/dockerfile-ros1-bridge-ros1-noetic-ros2-humble-on-l4t-jetpack-r36-4-0/331453
37. pcl_ros - ROS Wiki, accessed November 17, 2025, https://wiki.ros.org/pcl_ros
38. How do I build a custom container? · Issue #350 · dusty-nv/jetson-containers - GitHub, accessed November 17, 2025, https://github.com/dusty-nv/jetson-containers/issues/350

